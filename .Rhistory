)
# Paso 1: reconstruir los bloques desde matty_clean
raw_blocks <- strsplit(matty_clean, '\\},\\{"id":"')[[1]]
blocks <- mapply(function(i, x) {
if (i == 1) {
paste0("{", x, "}")
} else {
paste0('{"id":"', x, "}")
}
}, seq_along(raw_blocks), raw_blocks)
# Función segura para extraer un campo
extract_field <- function(block, field) {
pattern <- paste0('"', field, '":"?([^",}]+)"?')
match <- str_match(block, pattern)
if (!is.na(match[2])) return(match[2]) else return(NA)
}
# Extraer name y marker_date
df <- data.frame(
name = sapply(blocks, extract_field, "name"),
marker_date = sapply(blocks, extract_field, "marker_date"),
stringsAsFactors = FALSE
)
# Verifica que sí se extrajeron los campos
print(head(df, 5))
# Filtrar por año 2024, 2025 o "avg" en name
df_filtered <- df %>%
filter(
str_detect(tolower(marker_date), "2024|2025") |
str_detect(tolower(name), "avg")
)
cat(blocks[1])
matty_clean
library(rvest)
library(stringr)
library(dplyr)
# Paso 1: Descarga el HTML
url <- "https://www.worldcubeassociation.org/persons/2016INAB01"
page <- read_html(url)
# Paso 2: Extrae los scripts que contengan 'competitions' o 'marker_date'
scripts <- page %>% html_nodes("script") %>% html_text()
script_json <- scripts[grep("marker_date", scripts)[1]]  # toma el primero que contenga datos
# Paso 3: Extraer el contenido de JSON crudo que está en el script
# Suponiendo que haya un array: var COMPETITIONS = [ {...}, {...}, ... ];
json_raw <- str_extract(script_json, "\\[\\{.*\\}\\]")
# Paso 4: Si json_raw extrae bien, puedes parsearlo con jsonlite
library(jsonlite)
df_all <- fromJSON(json_raw, flatten = TRUE)
# Paso 5: Filtrar competencias de 2024, 2025 o con "avg" en el 'name'
df_filtered <- df_all %>%
filter(str_detect(marker_date, "2024|2025") |
str_detect(tolower(name), "avg"))
# Paso 6: Mostrar resultado
print(df_filtered)
View(df)
View(df)
View(df_all)
write.csv(df_all, 'matty.csv')
# Paso 1: Descarga el HTML
url <- "https://www.worldcubeassociation.org/persons/2021ZHAN01"
page <- read_html(url)
# Paso 2: Extrae los scripts que contengan 'competitions' o 'marker_date'
scripts <- page %>% html_nodes("script") %>% html_text()
script_json <- scripts[grep("marker_date", scripts)[1]]  # toma el primero que contenga datos
# Paso 3: Extraer el contenido de JSON crudo que está en el script
# Suponiendo que haya un array: var COMPETITIONS = [ {...}, {...}, ... ];
json_raw <- str_extract(script_json, "\\[\\{.*\\}\\]")
# Paso 4: Si json_raw extrae bien, puedes parsearlo con jsonlite
library(jsonlite)
df_all <- fromJSON(json_raw, flatten = TRUE)
# Paso 5: Filtrar competencias de 2024, 2025 o con "avg" en el 'name'
df_filtered <- df_all %>%
filter(str_detect(marker_date, "2024|2025") |
str_detect(tolower(name), "avg"))
# Paso 6: Mostrar resultado
print(df_filtered)
write.csv(df_all, 'bofan.csv')
# Paso 1: Descarga el HTML
url <- "https://www.worldcubeassociation.org/persons/2017GARR05"
page <- read_html(url)
# Paso 2: Extrae los scripts que contengan 'competitions' o 'marker_date'
scripts <- page %>% html_nodes("script") %>% html_text()
script_json <- scripts[grep("marker_date", scripts)[1]]  # toma el primero que contenga datos
# Paso 3: Extraer el contenido de JSON crudo que está en el script
# Suponiendo que haya un array: var COMPETITIONS = [ {...}, {...}, ... ];
json_raw <- str_extract(script_json, "\\[\\{.*\\}\\]")
# Paso 4: Si json_raw extrae bien, puedes parsearlo con jsonlite
library(jsonlite)
df_all <- fromJSON(json_raw, flatten = TRUE)
# Paso 5: Filtrar competencias de 2024, 2025 o con "avg" en el 'name'
df_filtered <- df_all %>%
filter(str_detect(marker_date, "2024|2025") |
str_detect(tolower(name), "avg"))
# Paso 6: Mostrar resultado
print(df_filtered)
write.csv(df_all, 'luke.csv')
# Paso 1: Descarga el HTML
url <- "https://www.worldcubeassociation.org/persons/2021ZAJD03"
page <- read_html(url)
# Paso 2: Extrae los scripts que contengan 'competitions' o 'marker_date'
scripts <- page %>% html_nodes("script") %>% html_text()
script_json <- scripts[grep("marker_date", scripts)[1]]  # toma el primero que contenga datos
# Paso 3: Extraer el contenido de JSON crudo que está en el script
# Suponiendo que haya un array: var COMPETITIONS = [ {...}, {...}, ... ];
json_raw <- str_extract(script_json, "\\[\\{.*\\}\\]")
# Paso 4: Si json_raw extrae bien, puedes parsearlo con jsonlite
library(jsonlite)
df_all <- fromJSON(json_raw, flatten = TRUE)
# Paso 5: Filtrar competencias de 2024, 2025 o con "avg" en el 'name'
df_filtered <- df_all %>%
filter(str_detect(marker_date, "2024|2025") |
str_detect(tolower(name), "avg"))
# Paso 6: Mostrar resultado
print(df_filtered)
write.csv(df_all, 'teodor.csv')
library(rvest)
library(stringr)
library(dplyr)
# Paso 1: Descarga el HTML
url <- "https://www.worldcubeassociation.org/persons/2012PARK03"
page <- read_html(url)
# Paso 2: Extrae los scripts que contengan 'competitions' o 'marker_date'
scripts <- page %>% html_nodes("script") %>% html_text()
script_json <- scripts[grep("marker_date", scripts)[1]]  # toma el primero que contenga datos
# Paso 3: Extraer el contenido de JSON crudo que está en el script
# Suponiendo que haya un array: var COMPETITIONS = [ {...}, {...}, ... ];
json_raw <- str_extract(script_json, "\\[\\{.*\\}\\]")
# Paso 4: Si json_raw extrae bien, puedes parsearlo con jsonlite
library(jsonlite)
df_all <- fromJSON(json_raw, flatten = TRUE)
# Paso 5: Filtrar competencias de 2024, 2025 o con "avg" en el 'name'
df_filtered <- df_all %>%
filter(str_detect(marker_date, "2024|2025") |
str_detect(tolower(name), "avg"))
# Paso 6: Mostrar resultado
print(df_filtered)
write.csv(df_all, 'max.csv')
write.csv(df_all, 'ruihang.csv')
library(rvest)
library(stringr)
library(dplyr)
# Paso 1: Descarga el HTML
url <- "https://www.worldcubeassociation.org/persons/2017XURU04"
page <- read_html(url)
# Paso 2: Extrae los scripts que contengan 'competitions' o 'marker_date'
scripts <- page %>% html_nodes("script") %>% html_text()
script_json <- scripts[grep("marker_date", scripts)[1]]  # toma el primero que contenga datos
# Paso 3: Extraer el contenido de JSON crudo que está en el script
# Suponiendo que haya un array: var COMPETITIONS = [ {...}, {...}, ... ];
json_raw <- str_extract(script_json, "\\[\\{.*\\}\\]")
# Paso 4: Si json_raw extrae bien, puedes parsearlo con jsonlite
library(jsonlite)
df_all <- fromJSON(json_raw, flatten = TRUE)
# Paso 5: Filtrar competencias de 2024, 2025 o con "avg" en el 'name'
df_filtered <- df_all %>%
filter(str_detect(marker_date, "2024|2025") |
str_detect(tolower(name), "avg"))
# Paso 6: Mostrar resultado
print(df_filtered)
write.csv(df_all, 'ruihang.csv')
library(rvest)
library(stringr)
library(dplyr)
# Paso 1: Descarga el HTML
url <- "https://www.worldcubeassociation.org/persons/2016KOLA02"
page <- read_html(url)
# Paso 2: Extrae los scripts que contengan 'competitions' o 'marker_date'
scripts <- page %>% html_nodes("script") %>% html_text()
script_json <- scripts[grep("marker_date", scripts)[1]]  # toma el primero que contenga datos
# Paso 3: Extraer el contenido de JSON crudo que está en el script
# Suponiendo que haya un array: var COMPETITIONS = [ {...}, {...}, ... ];
json_raw <- str_extract(script_json, "\\[\\{.*\\}\\]")
# Paso 4: Si json_raw extrae bien, puedes parsearlo con jsonlite
library(jsonlite)
df_all <- fromJSON(json_raw, flatten = TRUE)
# Paso 1: Descarga el HTML
url <- "https://www.worldcubeassociation.org/persons/2016KOLA02"
page <- read_html(url)
# Paso 2: Extrae los scripts que contengan 'competitions' o 'marker_date'
scripts <- page %>% html_nodes("script") %>% html_text()
script_json <- scripts[grep("marker_date", scripts)[1]]  # toma el primero que contenga datos
# Paso 3: Extraer el contenido de JSON crudo que está en el script
# Suponiendo que haya un array: var COMPETITIONS = [ {...}, {...}, ... ];
json_raw <- str_extract(script_json, "\\[\\{.*\\}\\]")
# Paso 4: Si json_raw extrae bien, puedes parsearlo con jsonlite
library(jsonlite)
df_all <- fromJSON(json_raw, flatten = TRUE)
script json
script_json
df_all
cat(script_json)  # revisa el contenido del script completo
script_json <- scripts[grep("competition", scripts)[1]]
cat(script_json)
scripts <- page %>% html_nodes("script") %>% html_text()
script_json <- scripts[grep("marker_date", scripts)[1]]  # toma el primero que contenga datos
# Paso 3: Extraer el contenido de JSON crudo que está en el script
# Suponiendo que haya un array: var COMPETITIONS = [ {...}, {...}, ... ];
json_raw <- str_extract(script_json, "\\[\\{.*?\\}\\]")  # modo no-greedy
# Paso 4: Si json_raw extrae bien, puedes parsearlo con jsonlite
library(jsonlite)
df_all <- fromJSON(json_raw, flatten = TRUE)
# Paso 4: Si json_raw extrae bien, puedes parsearlo con jsonlite
library(jsonlite)
df_all <- fromJSON(json_raw)#, flatten = TRUE)
json_raw <- str_extract(script_json, "\\[\\{.*\\}\\]")
page <- read_html(url)
# Extraer todos los <script>
scripts <- page %>% html_elements("script") %>% html_text()
# Ver cuántos scripts hay
length(scripts)
# Buscar uno que tenga "competition"
grep("competition", scripts, value = TRUE)
# Paso 1: Descarga el HTML
url <- "https://www.worldcubeassociation.org/persons/2023GENG02"
page <- read_html(url)
# Paso 2: Extrae los scripts que contengan 'competitions' o 'marker_date'
scripts <- page %>% html_nodes("script") %>% html_text()
script_json <- scripts[grep("marker_date", scripts)[1]]  # toma el primero que contenga datos
# Paso 3: Extraer el contenido de JSON crudo que está en el script
# Suponiendo que haya un array: var COMPETITIONS = [ {...}, {...}, ... ];
json_raw <- str_extract(script_json, "\\[\\{.*\\}\\]")
# Paso 4: Si json_raw extrae bien, puedes parsearlo con jsonlite
library(jsonlite)
df_all <- fromJSON(json_raw, flatten = TRUE)
# Paso 5: Filtrar competencias de 2024, 2025 o con "avg" en el 'name'
df_filtered <- df_all %>%
filter(str_detect(marker_date, "2024|2025") |
str_detect(tolower(name), "avg"))
# Paso 6: Mostrar resultado
print(df_filtered)
write.csv(df_all, 'xuanyi.csv')
url <- "https://www.worldcubeassociation.org/persons/2019WANY36"
page <- read_html(url)
# Paso 2: Extrae los scripts que contengan 'competitions' o 'marker_date'
scripts <- page %>% html_nodes("script") %>% html_text()
script_json <- scripts[grep("marker_date", scripts)[1]]  # toma el primero que contenga datos
# Paso 3: Extraer el contenido de JSON crudo que está en el script
# Suponiendo que haya un array: var COMPETITIONS = [ {...}, {...}, ... ];
json_raw <- str_extract(script_json, "\\[\\{.*\\}\\]")
# Paso 4: Si json_raw extrae bien, puedes parsearlo con jsonlite
library(jsonlite)
df_all <- fromJSON(json_raw, flatten = TRUE)
# Paso 5: Filtrar competencias de 2024, 2025 o con "avg" en el 'name'
df_filtered <- df_all %>%
filter(str_detect(marker_date, "2024|2025") |
str_detect(tolower(name), "avg"))
# Paso 6: Mostrar resultado
print(df_filtered)
write.csv(df_all, 'yiheng.csv')
url <- "https://www.worldcubeassociation.org/persons/2016KOLA02"
page <- read_html(url)
# Paso 2: Extraer el script que contiene los datos de competencias
scripts <- page %>% html_elements("script") %>% html_text()
# Paso 3: Buscar el bloque que contiene los datos de competencias (usualmente contiene "competitions")
target_script <- scripts[which(str_detect(scripts, "competitions"))[1]]
# Paso 4: Extraer el bloque JSON que contiene la lista de competencias
# Este regex extrae contenido dentro de "competitions":[{...}]
json_text <- str_extract(target_script, '"competitions":\\[\\{.*?\\}\\]')
# Verifica que se haya extraído bien
if (is.na(json_text)) stop("No se encontró el bloque de competencias.")
library(rvest)
library(jsonlite)
library(dplyr)
library(stringr)
# Paso 1: Leer la página HTML
url <- "https://www.worldcubeassociation.org/persons/2016KOLA02"
page <- read_html(url)
# Paso 2: Extraer todos los scripts
scripts <- page %>% html_elements("script") %>% html_text()
# Paso 3: Buscar el script que contiene "competitions"
target_script <- scripts[which(str_detect(scripts, "competitions"))[1]]
# Paso 4: Extraer el bloque JSON
json_text <- str_extract(target_script, '"competitions":\\[\\{.*?\\}\\]')
# Verificar extracción válida
if (is.na(json_text)) stop("No se encontró el bloque de competencias.")
library(rvest)
library(jsonlite)
library(stringr)
library(dplyr)
# Paso 1: Leer HTML de la página
url <- "https://www.worldcubeassociation.org/persons/2016KOLA02"
page <- read_html(url)
# Paso 2: Extraer todos los <script>
scripts <- page %>% html_elements("script") %>% html_text()
# Paso 3: Buscar el script que contiene 'window._reactProps'
react_script <- scripts[which(str_detect(scripts, "window._reactProps"))[1]]
# Paso 4: Extraer el JSON entre window._reactProps = y el ;
json_text <- str_extract(react_script, "window._reactProps = \\{.*\\};")
# Limpiar: quitar encabezado y punto y coma
json_clean <- str_replace(json_text, "window._reactProps = ", "")
json_clean <- str_remove(json_clean, ";$")
# Paso 5: Convertir a lista R
data <- fromJSON(json_clean, flatten = TRUE)
# Verifica si se extrajo correctamente el bloque
length(scripts)                  # Cuántos scripts hay
which(str_detect(scripts, "window._reactProps"))  # Índice del script correcto
library(rvest)
library(jsonlite)
library(stringr)
library(dplyr)
# Leer HTML
url <- "https://www.worldcubeassociation.org/persons/2016KOLA02"
page <- read_html(url)
# Extraer scripts
scripts <- page %>% html_elements("script") %>% html_text()
# Buscar script que contenga window._reactProps
script_index <- which(str_detect(scripts, "window._reactProps"))
if (length(script_index) == 0) stop("❌ No se encontró el script con window._reactProps.")
library(jsonlite)
# ID del competidor (cambia por el que quieras)
wca_id <- "2016KOLA02"
# URL del endpoint de la API
url <- paste0("https://www.worldcubeassociation.org/api/v0/persons/", wca_id)
# Descargar JSON
data <- fromJSON(url, flatten = TRUE)
# Extraer competencias
competitions <- data$competition_results
# Guardar CSV
write.csv(data, 'tymon.csv')
library(jsonlite)
# ID del competidor (puedes cambiarlo por cualquier otro)
wca_id <- "2016KOLA02"
# URL del endpoint de la API
url <- paste0("https://www.worldcubeassociation.org/api/v0/persons/", wca_id)
# Descargar JSON desde la API
data <- fromJSON(url, flatten = TRUE)
# Ver estructura general (sin competencia)
general_info <- data[setdiff(names(data), "competition_results")]
# Mostrar por consola (puedes usar View(general_info) si estás en RStudio)
print(general_info)
# Guardar como CSV si quieres (solo los datos generales)
write.csv(as.data.frame(general_info), paste0("info_general_", wca_id, ".csv"), row.names = FALSE)
# Guardar como CSV si quieres (solo los datos generales)
write.csv(df_filtered, "tymon.csv" )
library(rvest)
library(stringr)
library(dplyr)
# Paso 1: Descarga el HTML
url <- "https://www.worldcubeassociation.org/persons/2019WANY36"
page <- read_html(url)
# Paso 2: Extrae los scripts que contengan 'competitions' o 'marker_date'
scripts <- page %>% html_nodes("script") %>% html_text()
script_json <- scripts[grep("marker_date", scripts)[1]]  # toma el primero que contenga datos
# Paso 3: Extraer el contenido de JSON crudo que está en el script
# Suponiendo que haya un array: var COMPETITIONS = [ {...}, {...}, ... ];
json_raw <- str_extract(script_json, "\\[\\{.*\\}\\]")
# Paso 4: Si json_raw extrae bien, puedes parsearlo con jsonlite
library(jsonlite)
df_all <- fromJSON(json_raw, flatten = TRUE)
# Paso 5: Filtrar competencias de 2024, 2025 o con "avg" en el 'name'
df_filtered <- df_all %>%
filter(str_detect(marker_date, "2024|2025") |
str_detect(tolower(name), "average"))
# Paso 6: Mostrar resultado
print(df_filtered)
write.csv(df_all, 'yiheng.csv')
# Paso 5: Filtrar competencias de 2024, 2025 o con "avg" en el 'name'
df_filtered <- df_all %>%
filter(
str_detect(tolower(name), "Average"))
library(rvest)
library(stringr)
library(dplyr)
# Paso 1: Descarga el HTML
url <- "https://www.worldcubeassociation.org/persons/2019WANY36"
page <- read_html(url)
# Paso 2: Extrae los scripts que contengan 'competitions' o 'marker_date'
scripts <- page %>% html_nodes("script") %>% html_text()
script_json <- scripts[grep("marker_date", scripts)[1]]  # toma el primero que contenga datos
# Paso 3: Extraer el contenido de JSON crudo que está en el script
# Suponiendo que haya un array: var COMPETITIONS = [ {...}, {...}, ... ];
json_raw <- str_extract(script_json, "\\[\\{.*\\}\\]")
# Paso 4: Si json_raw extrae bien, puedes parsearlo con jsonlite
library(jsonlite)
df_all <- fromJSON(json_raw, flatten = TRUE)
# Paso 5: Filtrar competencias de 2024, 2025 o con "avg" en el 'name'
df_filtered <- df_all %>%
filter(
str_detect(tolower(name), "Average"))
# Paso 6: Mostrar resultado
print(df_filtered)
write.csv(df_all, 'yiheng.csv')
library(jsonlite)
# ID del competidor (puedes reemplazarlo por cualquier otro)
wca_id <- "2016KOLA02"
# URL de la API de la WCA
url <- paste0("https://www.worldcubeassociation.org/api/v0/persons/", wca_id)
# Descargar y parsear JSON
data <- fromJSON(url, flatten = TRUE)
# Extraer tabla completa de competencias con todos los resultados
competition_results <- data$competition_results
# Guardar como CSV con todas las columnas
write.csv(competition_results, paste0("competencias_completas_", wca_id, ".csv"), row.names = FALSE)
# Confirmar
cat("✅ Se guardó el archivo 'competencias_completas_", wca_id, ".csv' con ", nrow(competition_results), " filas y ", ncol(competition_results), " columnas.\n", sep = "")
library(RSelenium)
install.packages(RSelenium)
install.packages("RSelenium")
install.packages("wdman")       # Si quieres más control sobre el driver
install.packages("dplyr")
install.packages("rvest")
#install.packages("RSelenium")
#install.packages("wdman")       # Si quieres más control sobre el driver
#install.packages("dplyr")
#install.packages("rvest")
library(RSelenium)
library(dplyr)
library(rvest)
# Paso 1: Iniciar RSelenium (esto lanza un navegador)
rD <- rsDriver(browser = "chrome", port = 4545L, chromever = "latest")
#install.packages("RSelenium")
#install.packages("wdman")       # Si quieres más control sobre el driver
#install.packages("dplyr")
#install.packages("rvest")
library(RSelenium)
library(dplyr)
library(rvest)
# Paso 1: Iniciar RSelenium (esto lanza un navegador)
rD <- rsDriver(browser = "chrome", port = 4545L, chromever = "latest")
install.packages("chromote")
library(chromote)
library(rvest)
library(dplyr)
# ID del competidor (puedes cambiarlo)
wca_id <- "2016KOLA02"
url <- paste0("https://www.worldcubeassociation.org/persons/2016KOLA02")
# Iniciar instancia de Chrome
b <- ChromoteSession$new()
# Cargar página
b$Page$navigate(url)
b$Page$loadEventFired() # Espera a que cargue la página
# Esperar un poco más por JavaScript
Sys.sleep(3)
# Extraer HTML renderizado
html <- b$DOM$getDocument()$root$nodeId
full_html <- b$DOM$getOuterHTML(nodeId = html)$outerHTML
page <- read_html(full_html)
# Extraer tabla de resultados
tabla <- page %>%
html_element("table#results") %>%
html_table()
library(chromote)
library(rvest)
library(dplyr)
# Competidor WCA
wca_id <- "2016KOLA02"
url <- paste0("https://www.worldcubeassociation.org/persons/", wca_id)
# Iniciar navegador virtual con Chromote
b <- ChromoteSession$new()
# Ir a la página
b$Page$navigate(url)
b$Page$loadEventFired()
# Esperar a que exista la tabla en el DOM
b$Runtime$evaluate(
expression = "new Promise(resolve => {
const check = () => {
if (document.querySelector('#results')) {
resolve(true);
} else {
setTimeout(check, 500);
}
};
check();
})",
awaitPromise = TRUE
)
library(chromote)
library(rvest)
library(dplyr)
# Competidor WCA
wca_id <- "2016KOLA02"
url <- paste0("https://www.worldcubeassociation.org/persons/", wca_id)
# Iniciar navegador virtual con Chromote
b <- ChromoteSession$new()
# Ir a la página
b$Page$navigate(url)
b$Page$loadEventFired()
# Esperar a que exista la tabla en el DOM
b$Runtime$evaluate(
expression = "new Promise(resolve => {
const check = () => {
if (document.querySelector('#results')) {
resolve(true);
} else {
setTimeout(check, 500);
}
};
check();
})",
awaitPromise = TRUE
)
library(rvest)
library(stringr)
library(dplyr)
# Paso 1: Descarga el HTML
url <- "https://www.worldcubeassociation.org/persons/2019WANY36"
page <- read_html(url)
# Paso 2: Extrae los scripts que contengan 'competitions' o 'marker_date'
scripts <- page %>% html_nodes("script") %>% html_text()
script_json <- scripts[grep("marker_date", scripts)[1]]  # toma el primero que contenga datos
# Paso 3: Extraer el contenido de JSON crudo que está en el script
# Suponiendo que haya un array: var COMPETITIONS = [ {...}, {...}, ... ];
json_raw <- str_extract(script_json, "\\[\\{.*\\}\\]")
# Paso 4: Si json_raw extrae bien, puedes parsearlo con jsonlite
library(jsonlite)
df_all <- fromJSON(json_raw, flatten = TRUE)
# Paso 5: Filtrar competencias de 2024, 2025 o con "avg" en el 'name'
df_filtered <- df_all %>%
filter(
str_detect(tolower(name), "Average"))
# Paso 6: Mostrar resultado
print(df_filtered)
write.csv(df_all, 'yiheng.csv')
